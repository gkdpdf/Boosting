{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2732cebf-82f2-4af4-8054-38a71041ea46",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3e82b2-c440-4df3-a001-7130f410bdcc",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning technique used for regression tasks, where the goal is to predict a continuous numeric output. It is an ensemble learning method that combines the predictions of multiple weak learners, typically decision trees, to create a stronger predictive model.\n",
    "\n",
    "The basic idea behind gradient boosting is to build a series of weak learners sequentially, with each learner correcting the errors made by the previous ones. The learning process is driven by the gradient of the loss function with respect to the predicted values. In each iteration, a new weak learner is trained to fit the negative gradient of the loss function. The predictions of all weak learners are then combined to produce the final prediction.\n",
    "\n",
    "Key components of Gradient Boosting Regression include:\n",
    "\n",
    "1. **Weak Learners (Decision Trees):** Decision trees are commonly used as weak learners in gradient boosting. These are shallow trees, often referred to as \"stumps\" or \"shallow trees,\" with a limited depth to avoid overfitting.\n",
    "\n",
    "2. **Loss Function:** The loss function measures the difference between the predicted values and the actual target values. Common loss functions for regression problems include mean squared error (MSE) and mean absolute error (MAE).\n",
    "\n",
    "3. **Gradient Descent:** The algorithm minimizes the loss function by iteratively updating the model. In each iteration, the negative gradient of the loss function is used to adjust the predictions.\n",
    "\n",
    "4. **Learning Rate:** A hyperparameter that controls the step size at each iteration. It scales the contribution of each weak learner to the final prediction. A smaller learning rate generally leads to a more robust model but requires more iterations.\n",
    "\n",
    "Gradient Boosting Regression is implemented in various libraries such as Scikit-Learn (with `GradientBoostingRegressor`), XGBoost, LightGBM, and CatBoost. These libraries offer efficient implementations and additional features to improve performance and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc9a9a8-151b-478e-a1d6-f24c5a3296d4",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9da2e3f5-6a68-46d8-af17-daedb5cd2d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Generate a simple synthetic dataset for regression\n",
    "np.random.seed(42)\n",
    "X = np.sort(5 * np.random.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "split_ratio = 0.8\n",
    "split_index = int(split_ratio * len(X))\n",
    "\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "# Define the decision stump as a weak learner\n",
    "class DecisionStump:\n",
    "    def __init__(self):\n",
    "        self.threshold = None\n",
    "        self.alpha = None\n",
    "        self.direction = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Find the threshold that minimizes the weighted mean squared error\n",
    "        min_error = float('inf')\n",
    "        for threshold in X:\n",
    "            left_indices = X[:, 0] < threshold\n",
    "            right_indices = ~left_indices\n",
    "\n",
    "            left_error = np.sum((y[left_indices] - np.mean(y[left_indices]))**2)\n",
    "            right_error = np.sum((y[right_indices] - np.mean(y[right_indices]))**2)\n",
    "\n",
    "            total_error = left_error + right_error\n",
    "\n",
    "            if total_error < min_error:\n",
    "                min_error = total_error\n",
    "                self.threshold = threshold\n",
    "                self.alpha = 0.5 * np.log((1 - min_error) / (min_error + 1e-10))\n",
    "\n",
    "                if left_error < right_error:\n",
    "                    self.direction = 1\n",
    "                else:\n",
    "                    self.direction = -1\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.alpha * self.direction * (X[:, 0] < self.threshold)\n",
    "\n",
    "# Gradient Boosting Regression\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=50):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.models = [DecisionStump() for _ in range(n_estimators)]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize predictions\n",
    "        predictions = np.zeros_like(y)\n",
    "\n",
    "        for model in self.models:\n",
    "            # Calculate the pseudo-residuals\n",
    "            residuals = y - predictions\n",
    "\n",
    "            # Fit the weak learner to the residuals\n",
    "            model.fit(X, residuals)\n",
    "\n",
    "            # Update predictions\n",
    "            predictions += model.predict(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros(X.shape[0])\n",
    "        for model in self.models:\n",
    "            predictions += model.predict(X)\n",
    "        return predictions\n",
    "\n",
    "# Train the Gradient Boosting model\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(X_test[:, 0], y_test, color='black', label='Actual')\n",
    "plt.scatter(X_test[:, 0], y_pred, color='red', label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Gradient Boosting Regression')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0378a2f5-869a-47a6-b93e-a55fd952e63d",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdc79bf1-c2ca-4b1e-a2dd-0b1172ccc633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 200}\n",
      "Mean Squared Error (Best Model): 0.04423798247681965\n",
      "R-squared (Best Model): -0.8750755687430751\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Define the Gradient Boosting model\n",
    "gb_model = GradientBoostingRegressor()\n",
    "\n",
    "# Define the hyperparameters grid to search\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Use GridSearchCV with cross-validation\n",
    "grid_search = GridSearchCV(estimator=gb_model, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the best model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (Best Model): {mse}\")\n",
    "print(f\"R-squared (Best Model): {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09432076-6ba0-445a-8088-46ffb8931b79",
   "metadata": {},
   "source": [
    "## 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ab3f3f-ff4e-47cb-a788-5cbe80865912",
   "metadata": {},
   "source": [
    "In the context of Gradient Boosting, a weak learner is a model that performs slightly better than random chance on a given problem. More formally, a weak learner is a model that has limited predictive power, and it doesn't need to be highly accurate on its own. The concept of weak learners is fundamental to the success of ensemble methods like Gradient Boosting.\n",
    "\n",
    "For regression problems in Gradient Boosting, decision trees are often used as weak learners. These are typically shallow trees with a limited number of nodes or depth. Shallow trees are less complex and have weaker predictive power compared to deep trees. Each decision tree in the ensemble focuses on capturing a specific pattern or relationship within the data.\n",
    "\n",
    "The strength of Gradient Boosting lies in its ability to sequentially add weak learners to the ensemble, where each new learner corrects the errors made by the combination of the existing learners. During the training process, the algorithm fits the weak learner to the residuals (the differences between the actual and predicted values) from the previous iterations, adjusting the model to minimize the overall error.\n",
    "\n",
    "The key characteristics of a weak learner in the context of Gradient Boosting include:\n",
    "\n",
    "1. **Limited Complexity:** Weak learners are intentionally kept simple and less expressive. In the case of decision trees, this often means shallow trees with few nodes.\n",
    "\n",
    "2. **Slightly Better than Random:** A weak learner doesn't need to be highly accurate, but it should perform slightly better than random chance. This ensures that each learner contributes some meaningful information to the ensemble.\n",
    "\n",
    "3. **Sequential Improvement:** The ensemble's strength comes from the sequential addition of weak learners, each improving upon the mistakes of its predecessors.\n",
    "\n",
    "Common examples of weak learners in addition to shallow decision trees include linear models, simple rules, or even small neural networks. The choice of weak learner depends on the specific problem and the characteristics of the data. The power of Gradient Boosting comes from its ability to effectively combine these weak learners into a strong predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db7b29d-19ad-4944-8e67-d83fe3b24932",
   "metadata": {},
   "source": [
    "## 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556497e7-dd43-4b53-9987-543cb36f465f",
   "metadata": {},
   "source": [
    "The intuition behind the Gradient Boosting algorithm can be understood through the metaphor of a team of experts collaborating to solve a problem. Here's a simplified explanation:\n",
    "\n",
    "1. **Initialization:**\n",
    "   - Imagine you have a problem to solve, and you need to make predictions.\n",
    "   - In the beginning, you have a weak learner (like a novice expert) who makes predictions. However, these predictions are not very accurate.\n",
    "\n",
    "2. **Learning from Mistakes:**\n",
    "   - The algorithm examines the mistakes made by the weak learner. It calculates the differences between the predicted values and the actual values (residuals).\n",
    "   - The next weak learner is then trained to focus on correcting these mistakes. It learns to predict the residuals left by the previous expert.\n",
    "\n",
    "3. **Ensemble of Experts:**\n",
    "   - Now, you have two weak learners. They each have their strengths and weaknesses, but together they can provide better predictions than each alone.\n",
    "   - The algorithm combines the predictions of both learners to get an improved result.\n",
    "\n",
    "4. **Iterative Improvement:**\n",
    "   - The process continues iteratively. In each iteration, a new weak learner is introduced to correct the remaining errors from the combined predictions of the existing ensemble.\n",
    "   - Each weak learner is like a new expert joining the team, specializing in the aspects of the problem that the ensemble finds challenging.\n",
    "\n",
    "5. **Weighted Collaboration:**\n",
    "   - The predictions of each weak learner are given a weight based on their performance. Learners that perform well get higher weight, and those that struggle get lower weight.\n",
    "   - The algorithm assigns weights in a way that minimizes the overall error when combining their predictions.\n",
    "\n",
    "6. **Final Prediction:**\n",
    "   - The final prediction is made by combining the predictions of all weak learners, each contributing proportionally based on their assigned weights.\n",
    "   - The ensemble of weak learners acts as a strong team, where each member contributes their expertise to solve the problem collectively.\n",
    "\n",
    "The intuition is that by sequentially adding weak learners and focusing on the mistakes of the ensemble, Gradient Boosting builds a robust and accurate predictive model. Each new weak learner corrects the errors made by the existing ensemble, gradually improving the model's performance. The weighted collaboration ensures that more emphasis is placed on the strengths of each learner, leading to a powerful and flexible predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99712156-0608-4fb5-b702-3f9e95cff198",
   "metadata": {},
   "source": [
    "## 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b0df67-8847-411f-808a-d68ba7baeb38",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners in a sequential manner, each weak learner correcting the errors made by the combination of the existing learners. The process can be summarized in the following steps:\n",
    "\n",
    "1. **Initialization:**\n",
    "   - The algorithm starts with a simple model, usually a decision stump (a shallow decision tree with only a few nodes).\n",
    "   - The initial prediction is the mean (or another appropriate value) of the target variable.\n",
    "\n",
    "2. **Compute Pseudo-Residuals:**\n",
    "   - Calculate the residuals by taking the difference between the actual target values and the current predictions.\n",
    "   - These residuals represent the errors made by the current model.\n",
    "\n",
    "3. **Train a Weak Learner:**\n",
    "   - Fit a new weak learner (e.g., decision tree) to the residuals. This new learner focuses on capturing the patterns or relationships that the current model failed to capture.\n",
    "   - The goal is to find the best weak learner that minimizes the residuals' error.\n",
    "\n",
    "4. **Update Predictions:**\n",
    "   - Combine the predictions of the new weak learner with the current predictions. The combination is done by adding a fraction (learning rate) of the weak learner's predictions to the current predictions.\n",
    "   - The learning rate controls the contribution of each weak learner to the overall ensemble. A smaller learning rate generally leads to a more robust model.\n",
    "\n",
    "5. **Repeat:**\n",
    "   - Repeat steps 2-4 for a predefined number of iterations or until a certain level of performance is achieved.\n",
    "   - In each iteration, a new weak learner is added to the ensemble, focusing on the mistakes made by the current ensemble.\n",
    "\n",
    "6. **Final Prediction:**\n",
    "   - The final prediction is the sum of the predictions from all weak learners in the ensemble.\n",
    "   - The sequential addition of weak learners ensures that the model becomes increasingly capable of capturing complex relationships in the data.\n",
    "\n",
    "The key idea is that each weak learner is responsible for addressing specific aspects of the problem that the existing ensemble finds challenging. The algorithm assigns weights to the weak learners based on their performance, and these weights determine the influence of each learner in the final prediction. The ensemble benefits from the strengths of individual learners and gradually improves its predictive power through the correction of errors in each iteration. This process results in a highly accurate and flexible model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54e684f-4a76-4910-8870-e61532acdce5",
   "metadata": {},
   "source": [
    "## 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370e02da-9233-4a17-8fbd-9741ee960538",
   "metadata": {},
   "source": [
    "Constructing the mathematical intuition behind the Gradient Boosting algorithm involves understanding the optimization process and the mathematical expressions used to update the model at each iteration. Below are the key steps involved in building the mathematical intuition of the Gradient Boosting algorithm:\n",
    "\n",
    "1. **Objective Function:**\n",
    "   - Define an objective function that needs to be optimized. For regression problems, the most common objective function is the mean squared error (MSE). The goal is to minimize this error.\n",
    "\n",
    "   \\[ \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2 \\]\n",
    "\n",
    "   where \\( N \\) is the number of samples, \\( y_i \\) is the actual target value, and \\( \\hat{y}_i \\) is the predicted value.\n",
    "\n",
    "2. **Initialize Model:**\n",
    "   - Start with an initial model, often a simple one like the mean of the target values. The initial model is denoted as \\( F_0(x) \\).\n",
    "\n",
    "   \\[ F_0(x) = \\text{initial prediction} \\]\n",
    "\n",
    "3. **Compute Pseudo-Residuals:**\n",
    "   - Compute the pseudo-residuals, which represent the negative gradient of the objective function with respect to the current predictions.\n",
    "\n",
    "   \\[ \\text{Pseudo-Residuals} = - \\frac{\\partial \\text{MSE}}{\\partial \\hat{y}_i} \\]\n",
    "\n",
    "   These pseudo-residuals represent the errors made by the current model.\n",
    "\n",
    "4. **Train Weak Learner:**\n",
    "   - Fit a weak learner (e.g., decision tree) to the pseudo-residuals. The weak learner is trained to predict the negative gradient, essentially learning to correct the errors made by the current model.\n",
    "\n",
    "   \\[ h_i(x) = \\text{weak learner}(x; \\theta_i) \\]\n",
    "\n",
    "   where \\( \\theta_i \\) are the parameters of the weak learner.\n",
    "\n",
    "5. **Update Model:**\n",
    "   - Update the current model by adding a fraction (learning rate, \\( \\eta \\)) of the weak learner's predictions to the current predictions.\n",
    "\n",
    "   \\[ F_{\\text{new}}(x) = F_{\\text{old}}(x) + \\eta h_i(x) \\]\n",
    "\n",
    "   The learning rate controls the step size of the updates and prevents overfitting.\n",
    "\n",
    "6. **Repeat:**\n",
    "   - Repeat steps 3-5 for a predefined number of iterations or until convergence. In each iteration, a new weak learner is trained to correct the errors made by the current ensemble.\n",
    "\n",
    "7. **Final Prediction:**\n",
    "   - The final prediction is the sum of the predictions from all weak learners.\n",
    "\n",
    "   \\[ \\hat{y}(x) = F_0(x) + \\eta \\sum_{i=1}^{N} h_i(x) \\]\n",
    "\n",
    "   The ensemble benefits from the strengths of individual weak learners, and the learning rate determines their influence on the final prediction.\n",
    "\n",
    "8. **Regularization (Optional):**\n",
    "   - Regularization techniques, such as tree pruning or shrinkage, may be applied to prevent overfitting and improve generalization.\n",
    "\n",
    "Understanding these mathematical steps provides insight into how Gradient Boosting optimizes the model iteratively by focusing on the errors made by the current ensemble and correcting them with the addition of weak learners. The learning rate controls the contribution of each weak learner, and the ensemble gradually improves its predictive power through sequential updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d244f107-e301-4d6c-9348-d69cf23f1ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
